{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION OF THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from nuscenes import NuScenes\n",
    "import os\n",
    "import json\n",
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import transform_matrix\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.eval.prediction.config import load_prediction_config\n",
    "from nuscenes.eval.prediction.compute_metrics import compute_metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.eval.prediction.data_classes import Prediction\n",
    "from nuscenes.eval.prediction.compute_metrics import compute_metrics\n",
    "from nuscenes.prediction.models.physics import ConstantVelocityHeading, PhysicsOracle\n",
    "from nuscenes.eval.prediction.data_classes import Prediction\n",
    "\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.eval.prediction.data_classes import Prediction\n",
    "from nuscenes.eval.prediction.metrics import MinADEK, MinFDEK, RowMean\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nuscenes.eval.prediction.config import load_prediction_config\n",
    "from nuscenes.eval.prediction.compute_metrics import compute_metrics\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
    "from nuscenes.prediction.input_representation.interface import InputRepresentation\n",
    "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
    "\n",
    "from nuscenes.eval.prediction.config import load_prediction_config\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.prediction.models.physics import ConstantVelocityHeading, PhysicsOracle\n",
    "\n",
    "from nuscenes.map_expansion import arcline_path_utils\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "from nuscenes.prediction.models.mtp import MTP\n",
    "from nuscenes.prediction.models.covernet import CoverNet\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nuscenes.prediction.models.mtp import MTPLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import NuScenesMTPInferenceDataset\n",
    "\n",
    "def generate_submission_notebook(model, dataset, output_path=\"submission.json\"):\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    \n",
    "    # Necesitamos el helper para buscar la pose del agente\n",
    "    helper = dataset.helper \n",
    "\n",
    "    print(f\" Generando submission con conversi贸n LOCAL -> GLOBAL...\")\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        img, agent_state, _, _ = dataset[i]\n",
    "        \n",
    "        # Recuperar tokens\n",
    "        raw_token = dataset.split[i]\n",
    "        instance_token, sample_token = raw_token.split(\"_\")\n",
    "\n",
    "        # Inferencia\n",
    "        img = img.unsqueeze(0)        \n",
    "        agent_state = agent_state.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(img, agent_state)\n",
    "\n",
    "        # Procesar salida (tu c贸digo de antes)\n",
    "        total_output_size = pred.shape[1]\n",
    "        num_modes = total_output_size // 25 \n",
    "        num_coords = num_modes * 24\n",
    "        \n",
    "        pred_coords = pred[0, :num_coords]\n",
    "        pred_probs = pred[0, num_coords:]\n",
    "        \n",
    "        # [Num_modos, 12, 2] en coordenadas LOCALES\n",
    "        pred_coords_local = pred_coords.reshape(num_modes, 12, 2).cpu().numpy()\n",
    "\n",
    "        # ============================================================\n",
    "        #  TRANSFORMACIN CRTICA: LOCAL -> GLOBAL\n",
    "        # ============================================================\n",
    "        \n",
    "        # 1. Obtener la pose actual del agente en el mapa global\n",
    "        sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "        translation = sample_annotation['translation'] # [x, y, z] global\n",
    "        rotation = sample_annotation['rotation']       # Quaternion global\n",
    "        \n",
    "        # 2. Convertir a matriz de transformaci贸n (Local -> Global)\n",
    "        # Nota: transform_matrix espera rotaci贸n como Quaternion y translaci贸n\n",
    "        # Pero ojo: MTP predice X,Y (2D). NuScenes es 3D.\n",
    "        \n",
    "        # Manera simplificada de rotar y trasladar vectores 2D:\n",
    "        quaternion = Quaternion(rotation)\n",
    "        \n",
    "        # Creamos un array vac铆o para las coordenadas globales\n",
    "        pred_coords_global = np.zeros_like(pred_coords_local)\n",
    "\n",
    "        for mode_idx in range(num_modes):\n",
    "            # Cogemos la trayectoria de un modo (Shape: 12, 2)\n",
    "            trajectory_local = pred_coords_local[mode_idx]\n",
    "            \n",
    "            # A. A帽adimos una columna de ceros para Z (necesario para rotaci贸n 3D)\n",
    "            # Shape se convierte en (12, 3) -> [x, y, 0]\n",
    "            traj_3d = np.hstack([trajectory_local, np.zeros((12, 1))])\n",
    "            \n",
    "            # B. Rotar (El agente mira hacia una direcci贸n, rotamos los puntos)\n",
    "            # Iteramos punto a punto o usamos vectorizaci贸n si es posible. \n",
    "            # rotate funciona con vector 煤nico, as铆 que iteramos para asegurar:\n",
    "            traj_rotated = np.array([quaternion.rotate(p) for p in traj_3d])\n",
    "            \n",
    "            # C. Trasladar (Sumar la posici贸n global actual del coche)\n",
    "            # Solo sumamos X e Y (铆ndices 0 y 1)\n",
    "            pred_coords_global[mode_idx, :, 0] = traj_rotated[:, 0] + translation[0]\n",
    "            pred_coords_global[mode_idx, :, 1] = traj_rotated[:, 1] + translation[1]\n",
    "            \n",
    "        # ============================================================\n",
    "\n",
    "        # Probabilidades\n",
    "        if num_modes > 1:\n",
    "            probs = torch.nn.functional.softmax(pred_probs, dim=0).cpu().numpy()\n",
    "        else:\n",
    "            probs = np.array([1.0])\n",
    "\n",
    "        prediction_obj = Prediction(\n",
    "            instance=instance_token,\n",
    "            sample=sample_token,\n",
    "            prediction=pred_coords_global, # 隆USAMOS LAS GLOBALES!\n",
    "            probabilities=probs\n",
    "        )\n",
    "\n",
    "        predictions_list.append(prediction_obj.serialize())\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(predictions_list, f, indent=2)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `evaluate_submission` loads a submission file, runs the official\n",
    "nuScenes evaluation pipeline, and stores the resulting metrics. Keeping this\n",
    "code inside `src/` allows us to evaluate any trained model with a single\n",
    "function call from our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import evaluate_submission\n",
    "from src.evaluate import load_model \n",
    "\n",
    "\n",
    "# IMPORTANTE: primero defines el modelo y el optimizador vac铆os\n",
    "backbone = ResNetBackbone('resnet50')\n",
    "model = MTP(backbone, num_modes=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Ahora cargas pesos + optimizer\n",
    "model, optimizer, epoch = load_model(model, optimizer,\n",
    "                                     filename=\"mtp_lane_loss_v1.pth\",\n",
    "                                     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_val = get_prediction_challenge_split(\"mini_val\", dataroot=DATAROOT)\n",
    "dataset_val = NuScenesMTPInferenceDataset(mini_val, helper, mtp_input_representation, device=device)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=False)\n",
    "len(mini_val)\n",
    "\n",
    "# Generar submission\n",
    "submission_path = generate_submission_notebook(model, dataset_val, \"submission_mtp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_submission(\"v1.0-mini\", DATAROOT, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION OF THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIONES AUXILIARES ---\n",
    "def meters_to_pixels(coords_meters, img_size=500, resolution=0.2, center_offset=(250, 250)):\n",
    "    \"\"\"\n",
    "    Convierte coordenadas locales (metros) a coordenadas de imagen (p铆xeles).\n",
    "    Asume que el agente est谩 en el centro de la imagen mirando hacia ARRIBA.\n",
    "    \n",
    "    Coords input: [Lateral (X), Adelante (Y)] seg煤n descubrimos en tu modelo.\n",
    "    \"\"\"\n",
    "    # Desempaquetamos\n",
    "    lateral_meters = coords_meters[:, 0]\n",
    "    forward_meters = coords_meters[:, 1]\n",
    "    \n",
    "    center_x, center_y = center_offset\n",
    "    \n",
    "    # Conversi贸n a p铆xeles\n",
    "    # X en imagen aumenta hacia la derecha. Lateral positivo es derecha.\n",
    "    pix_x = center_x + (lateral_meters / resolution)\n",
    "    \n",
    "    # Y en imagen aumenta hacia ABAJO. Forward positivo es ARRIBA (restamos).\n",
    "    pix_y = center_y - (forward_meters / resolution)\n",
    "    \n",
    "    return pix_x, pix_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Usamos los tokens y datos del caso de Overfitting\n",
    "# (Aseg煤rate de que estas variables instance_token y sample_token siguen definidas de la celda anterior)\n",
    "print(f\"Visualizando Agente: {instance_token} en Sample: {sample_token}\")\n",
    "\n",
    "# 2. Obtener la IMAGEN BASE (Rasterizada)\n",
    "# Usamos el c贸digo que t煤 propusiste\n",
    "img_raster = mtp_input_representation.make_input_representation(instance_token, sample_token)\n",
    "\n",
    "# 3. Obtener REALIDAD LOCAL (Ground Truth en metros, desde el helper)\n",
    "gt_local_meters = helper.get_future_for_agent(instance_token, sample_token, seconds=6, in_agent_frame=True)\n",
    "\n",
    "# 4. Obtener PREDICCIN LOCAL (Del modelo overfiteado)\n",
    "# Recuperamos los datos crudos del dataset para inferencia\n",
    "img_tensor, agent_tensor, _ = dataset_train[idx] # Usamos el mismo idx=0 del overfitting\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Inferencia\n",
    "    pred_raw = model(img_tensor.unsqueeze(0).to(device), agent_tensor.unsqueeze(0).to(device))\n",
    "\n",
    "# Desempaquetar (Asumimos 1 modo principal ya que est谩 overfiteado)\n",
    "num_modes = pred_raw.shape[1] // 25\n",
    "pred_coords_all = pred_raw[0, :num_modes*24].reshape(num_modes, 12, 2).cpu().numpy()\n",
    "pred_local_meters = pred_coords_all[0] # Tomamos el primer modo\n",
    "\n",
    "# 5. CONVERTIR A PXELES\n",
    "# Importante: el rasterizador est谩ndar suele usar resoluci贸n 0.2m/px (100m en 500px)\n",
    "gt_pix_x, gt_pix_y = meters_to_pixels(gt_local_meters, resolution=0.2)\n",
    "pred_pix_x, pred_pix_y = meters_to_pixels(pred_local_meters, resolution=0.2)\n",
    "\n",
    "print(\"Datos preparados. Listos para visualizar.\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# 1. Mostrar la imagen rasterizada de fondo\n",
    "plt.imshow(img_raster)\n",
    "\n",
    "# 2. Pintar el Agente (punto central)\n",
    "plt.plot(250, 250, 'yo', markersize=12, markeredgecolor='k', label='Agente (Inicio)')\n",
    "\n",
    "# 3. Pintar Realidad (Verde grueso)\n",
    "plt.plot(gt_pix_x, gt_pix_y, 'g.-', linewidth=4, markersize=8, alpha=0.7, label='Realidad (GT)')\n",
    "\n",
    "# 4. Pintar Predicci贸n (Rojo discontinuo encima)\n",
    "plt.plot(pred_pix_x, pred_pix_y, 'r.--', linewidth=3, markersize=8, label='Predicci贸n Modelo')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Vista de P谩jaro Local: Predicci贸n vs Realidad (Overfitting)\\n(Si sale perfecto, las l铆neas se solapan)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization but with video instead of foto: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(img_raster)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Animaci贸n Local: Predicci贸n (Rojo) vs Realidad (Verde)\")\n",
    "\n",
    "# Inicializar elementos vac铆os\n",
    "line_gt, = ax.plot([], [], 'g-', linewidth=3, alpha=0.6, label='Traza Real')\n",
    "point_gt, = ax.plot([], [], 'go', markersize=10, label='Posici贸n Real')\n",
    "\n",
    "line_pred, = ax.plot([], [], 'r--', linewidth=2, label='Traza Predicci贸n')\n",
    "point_pred, = ax.plot([], [], 'ro', markersize=10, label='Posici贸n Predicha')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "def init():\n",
    "    line_gt.set_data([], [])\n",
    "    point_gt.set_data([], [])\n",
    "    line_pred.set_data([], [])\n",
    "    point_pred.set_data([], [])\n",
    "    return line_gt, point_gt, line_pred, point_pred\n",
    "\n",
    "def update(frame):\n",
    "    # Frame va de 0 a 11 (los 12 pasos)\n",
    "    \n",
    "    # Actualizar Realidad\n",
    "    line_gt.set_data(gt_pix_x[:frame+1], gt_pix_y[:frame+1])\n",
    "    point_gt.set_data([gt_pix_x[frame]], [gt_pix_y[frame]])\n",
    "    \n",
    "    # Actualizar Predicci贸n\n",
    "    line_pred.set_data(pred_pix_x[:frame+1], pred_pix_y[:frame+1])\n",
    "    point_pred.set_data([pred_pix_x[frame]], [pred_pix_y[frame]])\n",
    "    \n",
    "    return line_gt, point_gt, line_pred, point_pred\n",
    "\n",
    "# Crear animaci贸n (interval=300ms para que vaya lento y se vea bien)\n",
    "anim = FuncAnimation(fig, update, frames=len(gt_pix_x), init_func=init, blit=True, interval=300)\n",
    "\n",
    "plt.close() # Cierra la figura est谩tica para no duplicar\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
