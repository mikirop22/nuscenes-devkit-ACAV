{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lane-Deviation-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtienes la referencia de la lane por la que circula el agente (map_api.get_lane) y se calcula en el entrenamiento la distancia del punto predicho a la polil√≠nea del carril. As√≠ si te sales castigas al modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTANCIA DE UN PUNTO A UN SEGMENTO\n",
    "import torch\n",
    "def point_to_segment_distance(p, a, b):\n",
    "    \"\"\"\n",
    "    p: tensor (..., 2), punto\n",
    "    a, b: tensores (2,), extremos del segmento\n",
    "    Devuelve distancia m√≠nima punto-segmento\n",
    "    \"\"\"\n",
    "    ap = p - a\n",
    "    ab = b - a\n",
    "    ab_norm = torch.sum(ab * ab)\n",
    "\n",
    "    t = torch.clamp(torch.sum(ap * ab) / (ab_norm + 1e-8), 0., 1.)\n",
    "    proj = a + t * ab\n",
    "    return torch.norm(p - proj, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTANCIA DE PUNTO A POLIL√çNEA\n",
    "\n",
    "def point_to_polyline_distance(p, polyline):\n",
    "    \"\"\"\n",
    "    p: tensor (..., 2)\n",
    "    polyline: tensor (N, 2)\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i in range(polyline.shape[0] - 1):\n",
    "        a = polyline[i]\n",
    "        b = polyline[i+1]\n",
    "        dist = point_to_segment_distance(p, a, b)\n",
    "        distances.append(dist)\n",
    "    return torch.stack(distances).min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULAR LA LOSS PARA UNA SOLA TRAYECTORIA\n",
    "\n",
    "def lane_deviation_loss_single(traj_global, lane_polyline):\n",
    "    \"\"\"\n",
    "    traj_global: tensor (T, 2)\n",
    "    lane_polyline: tensor (N, 2)\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for t in range(traj_global.shape[0]):\n",
    "        pt = traj_global[t]\n",
    "        dist = point_to_polyline_distance(pt, lane_polyline)\n",
    "        distances.append(dist)\n",
    "    return torch.stack(distances).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MTPLoss:\n",
    "    \"\"\" Computes the loss for the MTP model. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_modes: int,\n",
    "                 regression_loss_weight: float = 1.,\n",
    "                 angle_threshold_degrees: float = 5.,\n",
    "                 lane_loss_weight=1.0, \n",
    "                 helper=None):\n",
    "        \"\"\"\n",
    "        Inits MTP loss.\n",
    "        :param num_modes: How many modes are being predicted for each agent.\n",
    "        :param regression_loss_weight: Coefficient applied to the regression loss to\n",
    "            balance classification and regression performance.\n",
    "        :param angle_threshold_degrees: Minimum angle needed between a predicted trajectory\n",
    "            and the ground to consider it a match.\n",
    "        \"\"\"\n",
    "        self.num_modes = num_modes\n",
    "        self.num_location_coordinates_predicted = 2  # We predict x, y coordinates at each timestep.\n",
    "        self.regression_loss_weight = regression_loss_weight\n",
    "        self.angle_threshold = angle_threshold_degrees\n",
    "        self.lane_loss_weight = lane_loss_weight\n",
    "        self.helper = helper\n",
    "\n",
    "    def _get_trajectory_and_modes(self,\n",
    "                                  model_prediction: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Splits the predictions from the model into mode probabilities and trajectory.\n",
    "        :param model_prediction: Tensor of shape [batch_size, n_timesteps * n_modes * 2 + n_modes].\n",
    "        :return: Tuple of tensors. First item is the trajectories of shape [batch_size, n_modes, n_timesteps, 2].\n",
    "            Second item are the mode probabilities of shape [batch_size, num_modes].\n",
    "        \"\"\"\n",
    "        mode_probabilities = model_prediction[:, -self.num_modes:].clone()\n",
    "\n",
    "        desired_shape = (model_prediction.shape[0], self.num_modes, -1, self.num_location_coordinates_predicted)\n",
    "        trajectories_no_modes = model_prediction[:, :-self.num_modes].clone().reshape(desired_shape)\n",
    "\n",
    "        return trajectories_no_modes, mode_probabilities\n",
    "\n",
    "    @staticmethod\n",
    "    def _angle_between(ref_traj: torch.Tensor,\n",
    "                       traj_to_compare: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Computes the angle between the last points of the two trajectories.\n",
    "        The resulting angle is in degrees and is an angle in the [0; 180) interval.\n",
    "        :param ref_traj: Tensor of shape [n_timesteps, 2].\n",
    "        :param traj_to_compare: Tensor of shape [n_timesteps, 2].\n",
    "        :return: Angle between the trajectories.\n",
    "        \"\"\"\n",
    "\n",
    "        EPSILON = 1e-5\n",
    "\n",
    "        if (ref_traj.ndim != 2 or traj_to_compare.ndim != 2 or\n",
    "                ref_traj.shape[1] != 2 or traj_to_compare.shape[1] != 2):\n",
    "            raise ValueError('Both tensors should have shapes (-1, 2).')\n",
    "\n",
    "        if torch.isnan(traj_to_compare[-1]).any() or torch.isnan(ref_traj[-1]).any():\n",
    "            return 180. - EPSILON\n",
    "\n",
    "        traj_norms_product = float(torch.norm(ref_traj[-1]) * torch.norm(traj_to_compare[-1]))\n",
    "\n",
    "        # If either of the vectors described in the docstring has norm 0, return 0 as the angle.\n",
    "        if math.isclose(traj_norms_product, 0):\n",
    "            return 0.\n",
    "\n",
    "        # We apply the max and min operations below to ensure there is no value\n",
    "        # returned for cos_angle that is greater than 1 or less than -1.\n",
    "        # This should never be the case, but the check is in place for cases where\n",
    "        # we might encounter numerical instability.\n",
    "        dot_product = float(ref_traj[-1].dot(traj_to_compare[-1]))\n",
    "        angle = math.degrees(math.acos(max(min(dot_product / traj_norms_product, 1), -1)))\n",
    "\n",
    "        if angle >= 180:\n",
    "            return angle - EPSILON\n",
    "\n",
    "        return angle\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ave_l2_norms(tensor: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Compute the average of l2 norms of each row in the tensor.\n",
    "        :param tensor: Shape [1, n_timesteps, 2].\n",
    "        :return: Average l2 norm. Float.\n",
    "        \"\"\"\n",
    "        l2_norms = torch.norm(tensor, p=2, dim=2)\n",
    "        avg_distance = torch.mean(l2_norms)\n",
    "        return avg_distance.item()\n",
    "\n",
    "    def _compute_angles_from_ground_truth(self, target: torch.Tensor,\n",
    "                                          trajectories: torch.Tensor) -> List[Tuple[float, int]]:\n",
    "        \"\"\"\n",
    "        Compute angle between the target trajectory (ground truth) and the predicted trajectories.\n",
    "        :param target: Shape [1, n_timesteps, 2].\n",
    "        :param trajectories: Shape [n_modes, n_timesteps, 2].\n",
    "        :return: List of angle, index tuples.\n",
    "        \"\"\"\n",
    "        angles_from_ground_truth = []\n",
    "        for mode, mode_trajectory in enumerate(trajectories):\n",
    "            # For each mode, we compute the angle between the last point of the predicted trajectory for that\n",
    "            # mode and the last point of the ground truth trajectory.\n",
    "            angle = self._angle_between(target[0], mode_trajectory)\n",
    "\n",
    "            angles_from_ground_truth.append((angle, mode))\n",
    "        return angles_from_ground_truth\n",
    "\n",
    "    def _compute_best_mode(self,\n",
    "                           angles_from_ground_truth: List[Tuple[float, int]],\n",
    "                           target: torch.Tensor, trajectories: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Finds the index of the best mode given the angles from the ground truth.\n",
    "        :param angles_from_ground_truth: List of (angle, mode index) tuples.\n",
    "        :param target: Shape [1, n_timesteps, 2]\n",
    "        :param trajectories: Shape [n_modes, n_timesteps, 2]\n",
    "        :return: Integer index of best mode.\n",
    "        \"\"\"\n",
    "\n",
    "        # We first sort the modes based on the angle to the ground truth (ascending order), and keep track of\n",
    "        # the index corresponding to the biggest angle that is still smaller than a threshold value.\n",
    "        angles_from_ground_truth = sorted(angles_from_ground_truth)\n",
    "        max_angle_below_thresh_idx = -1\n",
    "        for angle_idx, (angle, mode) in enumerate(angles_from_ground_truth):\n",
    "            if angle <= self.angle_threshold:\n",
    "                max_angle_below_thresh_idx = angle_idx\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # We choose the best mode at random IF there are no modes with an angle less than the threshold.\n",
    "        if max_angle_below_thresh_idx == -1:\n",
    "            best_mode = random.randint(0, self.num_modes - 1)\n",
    "\n",
    "        # We choose the best mode to be the one that provides the lowest ave of l2 norms between the\n",
    "        # predicted trajectory and the ground truth, taking into account only the modes with an angle\n",
    "        # less than the threshold IF there is at least one mode with an angle less than the threshold.\n",
    "        else:\n",
    "            # Out of the selected modes above, we choose the final best mode as that which returns the\n",
    "            # smallest ave of l2 norms between the predicted and ground truth trajectories.\n",
    "            distances_from_ground_truth = []\n",
    "\n",
    "            for angle, mode in angles_from_ground_truth[:max_angle_below_thresh_idx + 1]:\n",
    "                norm = self._compute_ave_l2_norms(target - trajectories[mode, :, :])\n",
    "\n",
    "                distances_from_ground_truth.append((norm, mode))\n",
    "\n",
    "            distances_from_ground_truth = sorted(distances_from_ground_truth)\n",
    "            best_mode = distances_from_ground_truth[0][1]\n",
    "\n",
    "        return best_mode\n",
    "\n",
    "    def __call__(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the MTP loss on a batch.\n",
    "        The predictions are of shape [batch_size, n_ouput_neurons of last linear layer]\n",
    "        and the targets are of shape [batch_size, 1, n_timesteps, 2]\n",
    "        :param predictions: Model predictions for batch.\n",
    "        :param targets: Targets for batch.\n",
    "        :return: zero-dim tensor representing the loss on the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_losses = torch.Tensor().requires_grad_(True).to(predictions.device)\n",
    "        trajectories, modes = self._get_trajectory_and_modes(predictions)\n",
    "\n",
    "        for batch_idx in range(predictions.shape[0]):\n",
    "\n",
    "            angles = self._compute_angles_from_ground_truth(target=targets[batch_idx],\n",
    "                                                            trajectories=trajectories[batch_idx])\n",
    "\n",
    "            best_mode = self._compute_best_mode(angles,\n",
    "                                                target=targets[batch_idx],\n",
    "                                                trajectories=trajectories[batch_idx])\n",
    "\n",
    "            best_mode_trajectory = trajectories[batch_idx, best_mode, :].unsqueeze(0)\n",
    "\n",
    "            regression_loss = f.smooth_l1_loss(best_mode_trajectory, targets[batch_idx])\n",
    "\n",
    "            mode_probabilities = modes[batch_idx].unsqueeze(0)\n",
    "            best_mode_target = torch.tensor([best_mode], device=predictions.device)\n",
    "            classification_loss = f.cross_entropy(mode_probabilities, best_mode_target)\n",
    "\n",
    "            loss = classification_loss + self.regression_loss_weight * regression_loss\n",
    "            # ============================================================\n",
    "            # üõ£Ô∏è LANE DEVIATION LOSS\n",
    "            # ============================================================\n",
    "            if self.lane_loss_weight > 0 and self.helper is not None:\n",
    "\n",
    "                instance_token, sample_token = tokens[batch_idx]\n",
    "\n",
    "                ann = self.helper.get_sample_annotation(instance_token, sample_token)\n",
    "                agent_x, agent_y = ann[\"translation\"][:2]\n",
    "\n",
    "                lane_ids = self.helper.map_api.get_lane_ids_in_xy(agent_x, agent_y)\n",
    "\n",
    "                if len(lane_ids) > 0:\n",
    "                    lane_id = lane_ids[0]\n",
    "                    lane_poly = torch.tensor(\n",
    "                        self.helper.map_api.get_lane_centerline(lane_id)[:, :2],\n",
    "                        dtype=best_mode_trajectory.dtype,\n",
    "                        device=best_mode_trajectory.device\n",
    "                    )\n",
    "\n",
    "                    # Convert local -> global\n",
    "                    traj_local = best_mode_trajectory[0]   # (T,2)\n",
    "                    quat = Quaternion(ann[\"rotation\"])\n",
    "\n",
    "                    traj_rot = torch.tensor(\n",
    "                        [quat.rotate((p[0].item(), p[1].item(), 0.0))[:2] for p in traj_local],\n",
    "                        dtype=traj_local.dtype,\n",
    "                        device=traj_local.device\n",
    "                    )\n",
    "\n",
    "                    traj_global = traj_rot + torch.tensor([agent_x, agent_y], device=traj_rot.device)\n",
    "\n",
    "                    # Lane deviation loss\n",
    "                    lane_loss = lane_deviation_loss_single(traj_global, lane_poly)\n",
    "\n",
    "                    loss = loss + self.lane_loss_weight * lane_loss\n",
    "\n",
    "\n",
    "            batch_losses = torch.cat((batch_losses, loss.unsqueeze(0)), 0)\n",
    "\n",
    "        avg_loss = torch.mean(batch_losses)\n",
    "\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canviar quan es truca a la loss per posarli \n",
    "# una weight a la lane loss (canviar a mtp)\n",
    "\n",
    "loss_fn = MTPLoss(\n",
    "    num_modes=num_modes,\n",
    "    regression_loss_weight=1.0,\n",
    "    angle_threshold_degrees=5.,\n",
    "    lane_loss_weight=1.0,   # <-- nuevo\n",
    "    helper=helper           # <-- necesario\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Snap-to-Lane: \n",
    "\n",
    "after the training is done the snap to lane function is used in the prediction to predict the closest point that is IN the lane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyquaternion import Quaternion\n",
    "import numpy as np\n",
    "\n",
    "def get_agent_lane(helper, instance_token, sample_token):\n",
    "    # Posici√≥n del agente en coordenadas globales\n",
    "    annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "    agent_x, agent_y = annotation['translation'][:2]\n",
    "    \n",
    "    lanes = helper.map_api.get_lane_ids_in_xy(agent_x, agent_y)\n",
    "    if len(lanes) == 0:\n",
    "        return None  # No ha encontrado lane (raro, pero posible)\n",
    "    \n",
    "    # Devolvemos la primera para simplificar\n",
    "    return lanes[0]\n",
    "\n",
    "def get_lane_centerline(helper, lane_id):\n",
    "    record = helper.map_api.get_lane(lane_id)\n",
    "    lane_center = helper.map_api.get_lane_centerline(lane_id)\n",
    "    # lane_center es un array Nx2 con la polil√≠nea\n",
    "    return np.array(lane_center[:, :2])\n",
    "\n",
    "def project_point_to_polyline(point, polyline):\n",
    "    px, py = point\n",
    "    min_dist = float('inf')\n",
    "    closest_point = None\n",
    "    \n",
    "    for i in range(len(polyline) - 1):\n",
    "        p1 = polyline[i]\n",
    "        p2 = polyline[i+1]\n",
    "        \n",
    "        v = p2 - p1\n",
    "        w = point - p1\n",
    "        \n",
    "        t = np.dot(w, v) / (np.dot(v, v) + 1e-8)\n",
    "        t = np.clip(t, 0, 1)\n",
    "        \n",
    "        proj = p1 + t * v\n",
    "        dist = np.linalg.norm(point - proj)\n",
    "\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_point = proj\n",
    "            \n",
    "    return closest_point\n",
    "\n",
    "\n",
    "def snap_trajectory_to_lane(global_traj, helper, instance_token, sample_token):\n",
    "    ann = helper.get_sample_annotation(instance_token, sample_token)\n",
    "    x, y = ann[\"translation\"][:2]\n",
    "\n",
    "    lane_ids = helper.map_api.get_lane_ids_in_xy(x, y)\n",
    "    if len(lane_ids) == 0:\n",
    "        return global_traj  # no lane found\n",
    "\n",
    "    lane_id = lane_ids[0]\n",
    "    centerline = helper.map_api.get_lane_centerline(lane_id)[:, :2]\n",
    "\n",
    "    snapped = []\n",
    "    for point in global_traj:\n",
    "        snapped.append(project_point_to_polyline(point, centerline))\n",
    "    return np.array(snapped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add to the generate submision function to pass from the global to a lane aligned position\n",
    "\n",
    "pred_coords_global[mode_idx] = snap_trajectory_to_lane(\n",
    "    pred_coords_global[mode_idx],\n",
    "    helper,\n",
    "    instance_token,\n",
    "    sample_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_notebook(model, dataset, output_path=\"submission.json\"):\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    \n",
    "    # Necesitamos el helper para buscar la pose del agente\n",
    "    helper = dataset.helper \n",
    "\n",
    "    print(f\"üöó Generando submission con conversi√≥n LOCAL -> GLOBAL...\")\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        img, agent_state, _, _ = dataset[i]\n",
    "        \n",
    "        # Recuperar tokens\n",
    "        raw_token = dataset.split[i]\n",
    "        instance_token, sample_token = raw_token.split(\"_\")\n",
    "\n",
    "        # Inferencia\n",
    "        img = img.unsqueeze(0)        \n",
    "        agent_state = agent_state.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(img, agent_state)\n",
    "\n",
    "        # Procesar salida (tu c√≥digo de antes)\n",
    "        total_output_size = pred.shape[1]\n",
    "        num_modes = total_output_size // 25 \n",
    "        num_coords = num_modes * 24\n",
    "        \n",
    "        pred_coords = pred[0, :num_coords]\n",
    "        pred_probs = pred[0, num_coords:]\n",
    "        \n",
    "        # [Num_modos, 12, 2] en coordenadas LOCALES\n",
    "        pred_coords_local = pred_coords.reshape(num_modes, 12, 2).cpu().numpy()\n",
    "\n",
    "        # ============================================================\n",
    "        # üåç TRANSFORMACI√ìN CR√çTICA: LOCAL -> GLOBAL\n",
    "        # ============================================================\n",
    "        \n",
    "        # 1. Obtener la pose actual del agente en el mapa global\n",
    "        sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "        translation = sample_annotation['translation'] # [x, y, z] global\n",
    "        rotation = sample_annotation['rotation']       # Quaternion global\n",
    "        \n",
    "        # 2. Convertir a matriz de transformaci√≥n (Local -> Global)\n",
    "        # Nota: transform_matrix espera rotaci√≥n como Quaternion y translaci√≥n\n",
    "        # Pero ojo: MTP predice X,Y (2D). NuScenes es 3D.\n",
    "        \n",
    "        # Manera simplificada de rotar y trasladar vectores 2D:\n",
    "        quaternion = Quaternion(rotation)\n",
    "        \n",
    "        # Creamos un array vac√≠o para las coordenadas globales\n",
    "        pred_coords_global = np.zeros_like(pred_coords_local)\n",
    "\n",
    "        for mode_idx in range(num_modes):\n",
    "            # Cogemos la trayectoria de un modo (Shape: 12, 2)\n",
    "            trajectory_local = pred_coords_local[mode_idx]\n",
    "            \n",
    "            # A. A√±adimos una columna de ceros para Z (necesario para rotaci√≥n 3D)\n",
    "            # Shape se convierte en (12, 3) -> [x, y, 0]\n",
    "            traj_3d = np.hstack([trajectory_local, np.zeros((12, 1))])\n",
    "            \n",
    "            # B. Rotar (El agente mira hacia una direcci√≥n, rotamos los puntos)\n",
    "            # Iteramos punto a punto o usamos vectorizaci√≥n si es posible. \n",
    "            # rotate funciona con vector √∫nico, as√≠ que iteramos para asegurar:\n",
    "            traj_rotated = np.array([quaternion.rotate(p) for p in traj_3d])\n",
    "            \n",
    "            # C. Trasladar (Sumar la posici√≥n global actual del coche)\n",
    "            # Solo sumamos X e Y (√≠ndices 0 y 1)\n",
    "            pred_coords_global[mode_idx, :, 0] = traj_rotated[:, 0] + translation[0]\n",
    "            pred_coords_global[mode_idx, :, 1] = traj_rotated[:, 1] + translation[1]\n",
    "\n",
    "            # ============================================================\n",
    "            # üõ£Ô∏è SNAP-TO-LANE (GLOBAL ‚Üí LANE-ALIGNED)\n",
    "            # ============================================================\n",
    "            pred_coords_global[mode_idx] = snap_trajectory_to_lane(\n",
    "                pred_coords_global[mode_idx],\n",
    "                helper,\n",
    "                instance_token,\n",
    "                sample_token\n",
    "            )\n",
    "            \n",
    "        # ============================================================\n",
    "\n",
    "        # Probabilidades\n",
    "        if num_modes > 1:\n",
    "            probs = torch.nn.functional.softmax(pred_probs, dim=0).cpu().numpy()\n",
    "        else:\n",
    "            probs = np.array([1.0])\n",
    "\n",
    "        prediction_obj = Prediction(\n",
    "            instance=instance_token,\n",
    "            sample=sample_token,\n",
    "            prediction=pred_coords_global, # ¬°USAMOS LAS GLOBALES!\n",
    "            probabilities=probs\n",
    "        )\n",
    "\n",
    "        predictions_list.append(prediction_obj.serialize())\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(predictions_list, f, indent=2)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESTRINGIR EL ESPACIO DE PREDICCI√ìN (LANE CONDITIONED MTP)\n",
    "\n",
    "4Ô∏è‚É£ Restringir el espacio de predicci√≥n (Lane-conditioned MTP)\n",
    "\n",
    "En vez de dejar que el modelo prediga cualquier trayectoria libre, puedes:\n",
    "\n",
    "Generar modos condicionados por la estructura de la lane (ramas, salidas, giros).\n",
    "\n",
    "Hacer que cada ‚Äúmodo‚Äù siga una lane candidate.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "CoverNet + Lattice basado en lanes\n",
    "\n",
    "LaneGCN\n",
    "\n",
    "Wayformer con road graph\n",
    "\n",
    "Aqu√≠ el modelo pr√°cticamente solo puede elegir trayectorias v√°lidas por construcci√≥n.\n",
    "\n",
    "Ventaja:\n",
    "\n",
    "Es la soluci√≥n m√°s elegante acad√©micamente.\n",
    "\n",
    "Desventaja:\n",
    "\n",
    "M√°s trabajo de ingenier√≠a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FER M√âS INTERESSANT EL BIRD EYE VIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Darle la informaci√≥n de la lane (BEV o vector lanes)\n",
    "\n",
    "‚û°Ô∏è La opci√≥n que sugiere tu profe.\n",
    "‚û°Ô∏è Es buena porque el modelo aprende ‚Äúpor s√≠ mismo‚Äù la geometr√≠a del mapa.\n",
    "\n",
    "Formas de hacerlo:\n",
    "\n",
    "Raster BEV completo (lo que estamos montando ahora).\n",
    "\n",
    "Lanes vectorizadas (formato Trajectron++ / VectorNet).\n",
    "\n",
    "A√±adir polil√≠neas directamente como input a un GNN o MLP.\n",
    "\n",
    "Ventaja: no fuerza expl√≠citamente, solo ayuda.\n",
    "Desventaja: el modelo a veces puede seguir equivoc√°ndose."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
