{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI wayformer a partir d'embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer s'ha hagut de fer els embeddings i concatenarlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes de entrada al Transformer (que preparar√°s despu√©s)\n",
    "# bev_emb:      (batch_size, 1, D)\n",
    "# agent_embs:   (batch_size, N_agents, D)\n",
    "# lane_embs:    (batch_size, N_lanes, D)\n",
    "\n",
    "# y luego los concatenas:\n",
    "#¬†tokens = torch.cat([bev_emb, agent_embs, lane_embs], dim=1)  # (B, T, D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent embedding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CADA AQUEST T√â AIX√í: posici√≥n (x, y), velocidad, aceleraci√≥n, heading (√°ngulo), tama√±o (width, length),clase (car, pedestrian, bicycle‚Ä¶), historia pasada (opcional), estado actual (si est√° quieto o no, etc.)\n",
    "\n",
    "amb una dimensionalidad podriem tenir un feature vector aix√≠: [x, y, vx, vy, ax, ay, heading_sin, heading_cos, width, length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def extract_agent_features(helper, instance_token, sample_token):\n",
    "    ann = helper.get_sample_annotation(instance_token, sample_token)\n",
    "\n",
    "    # Position\n",
    "    x, y, _ = ann[\"translation\"]\n",
    "\n",
    "    # Velocity\n",
    "    vx, vy = helper.get_velocity_for_agent(instance_token, sample_token)\n",
    "\n",
    "    # Acceleration\n",
    "    ax, ay = helper.get_acceleration_for_agent(instance_token, sample_token)\n",
    "\n",
    "    # Heading (convert quaternion ‚Üí sin/cos)\n",
    "    quat = ann[\"rotation\"]  # [qw, qx, qy, qz]\n",
    "    yaw = Quaternion(quat).yaw_pitch_roll[0]\n",
    "    sin_yaw = np.sin(yaw)\n",
    "    cos_yaw = np.cos(yaw)\n",
    "\n",
    "    # Size\n",
    "    width, length, _ = ann[\"size\"]\n",
    "\n",
    "    # Feature vector\n",
    "    features = np.array([x, y, vx, vy, ax, ay, sin_yaw, cos_yaw, width, length], dtype=np.float32)\n",
    "\n",
    "    return features  # shape (10,)\n",
    "\n",
    "\n",
    "## PASSAR TOTS ELS AGENTS PER UN MLP PER TENIR AGENT EMBEDDING DE agent embedding (B, N_agents, D)\n",
    "class AgentEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=10, hidden_dim=128, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, agent_features):\n",
    "        \"\"\"\n",
    "        agent_features: (B, N_agents, 10)\n",
    "        returns: (B, N_agents, out_dim)\n",
    "        \"\"\"\n",
    "        return self.mlp(agent_features)\n",
    "\n",
    "MAX_AGENTS = 12  # como Waymo, puedes escoger 12‚Äì16\n",
    "\n",
    "def build_agent_embeddings(helper, sample_token, agent_encoder, device):\n",
    "    anns = helper.get_annotations_for_sample(sample_token)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for ann in anns:\n",
    "        instance_token = ann[\"instance_token\"]\n",
    "        f = extract_agent_features(helper, instance_token, sample_token)\n",
    "        features.append(f)\n",
    "\n",
    "        if len(features) == MAX_AGENTS:\n",
    "            break\n",
    "\n",
    "    # Padding\n",
    "    while len(features) < MAX_AGENTS:\n",
    "        features.append(np.zeros(10, dtype=np.float32))\n",
    "\n",
    "    features = torch.tensor(features, device=device)  # (MAX_AGENTS, 10)\n",
    "    features = features.unsqueeze(0)  # (1, MAX_AGENTS, 10)\n",
    "\n",
    "    # Agent Encoder\n",
    "    with torch.no_grad():  # or leave gradients if end-to-end\n",
    "        agent_emb = agent_encoder(features)  # (1, MAX_AGENTS, d_model)\n",
    "\n",
    "    return agent_emb  # shape (1, N_agents, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectiu √©s donat un sample_token lane_emb: (1, N_lanes, d_model). Cada lane a nuscenes es un polyline ([(x1, y1), (x2, y2), ..., (xk, yk)])\n",
    "cADA LANE TIENE Type,conexiones, direccion del trafico y curbatura\n",
    "\n",
    "Pero la m√≠nima base funcional para un Transformer es:\n",
    "\n",
    "‚úî samplear la polil√≠nea\n",
    "‚úî convertir cada punto a un feature\n",
    "‚úî agregar todos los puntos para crear un embedding por lane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenim les polylines m√©s properes al agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_lanes(helper, agent_x, agent_y, max_lanes=8):\n",
    "    \"\"\"\n",
    "    Returns up to max_lanes lane IDs near the agent.\n",
    "    \"\"\"\n",
    "    lane_ids = helper.map_api.get_lane_ids_in_xy(agent_x, agent_y)\n",
    "\n",
    "    # If too many lanes, keep closest N\n",
    "    if len(lane_ids) > max_lanes:\n",
    "        lane_ids = lane_ids[:max_lanes]\n",
    "\n",
    "    return lane_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treiem la polil√≠nea de cada agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lane_polyline(helper, lane_id):\n",
    "    # returns Nx2 array with (x, y)\n",
    "    poly = helper.map_api.get_lane_centerline(lane_id)[:, :2]\n",
    "    return poly  # shape (K, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim cada polil√≠nea en un conjunt de features (com es fa a implementacions com LaneGCN y Vectornet)\n",
    "dx, dy                 ‚Üí direcci√≥n del segmento\n",
    "length                 ‚Üí distancia\n",
    "norm direction         ‚Üí dx / length, dy / length\n",
    "curvature              ‚Üí change of angle (si queremos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def polyline_to_features(poly):\n",
    "    \"\"\"\n",
    "    poly: (K,2)\n",
    "    returns features for each segment: (K-1, 5)\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for i in range(len(poly) - 1):\n",
    "        x1, y1 = poly[i]\n",
    "        x2, y2 = poly[i+1]\n",
    "\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        length = np.sqrt(dx*dx + dy*dy) + 1e-6\n",
    "        nx = dx / length\n",
    "        ny = dy / length\n",
    "\n",
    "        # Feature per segment\n",
    "        feat = [dx, dy, length, nx, ny]\n",
    "        segments.append(feat)\n",
    "\n",
    "    return np.array(segments, dtype=np.float32)  # (K-1, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara MLP com abans per fer el lane encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegmentEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=5, hidden_dim=128, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, seg_features):\n",
    "        \"\"\"\n",
    "        seg_features: (num_segments, in_dim)\n",
    "        returns: (num_segments, out_dim)\n",
    "        \"\"\"\n",
    "        return self.mlp(seg_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_lane_embedding(segment_embs):\n",
    "    \"\"\"\n",
    "    segment_embs: (num_segments, d_model)\n",
    "    returns a single lane embedding: (d_model,)\n",
    "    \"\"\"\n",
    "    return segment_embs.mean(dim=0)  # or max(dim=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lane_embedding(helper, lane_id, lane_encoder, device):\n",
    "    # 1. Polyline\n",
    "    poly = get_lane_polyline(helper, lane_id)  # (K,2)\n",
    "\n",
    "    seg_feats = polyline_to_features(poly)     # (K-1, 5)\n",
    "    seg_feats = torch.tensor(seg_feats, device=device)\n",
    "\n",
    "    # 2. Encode each segment\n",
    "    seg_embs = lane_encoder(seg_feats)         # (K-1, d_model)\n",
    "\n",
    "    # 3. Aggregate segment embeddings\n",
    "    lane_emb = seg_embs.mean(dim=0)            # (d_model,)\n",
    "\n",
    "    return lane_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos embedings para todos los lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LANES = 8\n",
    "\n",
    "def build_lane_embeddings(helper, agent_x, agent_y, lane_encoder, device):\n",
    "    lane_ids = get_relevant_lanes(helper, agent_x, agent_y, max_lanes=MAX_LANES)\n",
    "\n",
    "    lane_embs = []\n",
    "    for lane_id in lane_ids:\n",
    "        emb = build_lane_embedding(helper, lane_id, lane_encoder, device)\n",
    "        lane_embs.append(emb)\n",
    "\n",
    "    # Padding\n",
    "    while len(lane_embs) < MAX_LANES:\n",
    "        lane_embs.append(torch.zeros(lane_embs[0].shape, device=device))\n",
    "\n",
    "    lane_embs = torch.stack(lane_embs, dim=0)  # (MAX_LANES, d_model)\n",
    "    lane_embs = lane_embs.unsqueeze(0)         # (1, MAX_LANES, d_model)\n",
    "\n",
    "    return lane_embs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado tendria que ser un sistema on el lane_emb :(1,N_lanes, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bird Eye View embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest embedding hauria de ser un tensor que resumeixi tota la escena. bev_emb: (1, 1, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imatge rasterizada en vista cenital ja est√† implementada a nuscnees: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_layer_rasterizer = StaticLayerRasterizer(helper)\n",
    "agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)\n",
    "mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())\n",
    "img = mtp_input_representation.make_input_representation(instance_token, sample_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imatge que produeix el multicanal d'unes dimensions com: (3, 224, 224)    o (3, 500, 500)  seg√∫n configuraci√≥n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aix√≤ se li pasa a una resnet (cnn) treiem un embedding i el reduim a d_model i ser√† el primer token del transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "class BEVBackbone(nn.Module):\n",
    "    def __init__(self, out_dim=128, arch=\"resnet18\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if arch == \"resnet18\":\n",
    "            base = models.resnet18(weights=None)\n",
    "            feat_dim = 512\n",
    "        elif arch == \"resnet50\":\n",
    "            base = models.resnet50(weights=None)\n",
    "            feat_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "        # Remove final FC layer, keep convolutional trunk\n",
    "        self.cnn = nn.Sequential(*list(base.children())[:-1])  # output: (B, feat_dim, 1, 1)\n",
    "\n",
    "        # MLP to project onto transformer dimension\n",
    "        self.proj = nn.Linear(feat_dim, out_dim)\n",
    "\n",
    "    def forward(self, bev_img):\n",
    "        \"\"\"\n",
    "        bev_img: (B, C, H, W)\n",
    "        returns: (B, out_dim)\n",
    "        \"\"\"\n",
    "        x = self.cnn(bev_img)         # (B, feat_dim, 1, 1)\n",
    "        x = x.view(x.size(0), -1)     # (B, feat_dim)\n",
    "        x = self.proj(x)              # (B, out_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVEncoder(nn.Module):\n",
    "    def __init__(self, input_representation, d_model=128, arch=\"resnet18\", device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.input_representation = input_representation\n",
    "        self.device = device\n",
    "\n",
    "        # CNN backbone to extract features\n",
    "        self.backbone = BEVBackbone(out_dim=d_model, arch=arch).to(device)\n",
    "\n",
    "    def forward(self, instance_token, sample_token):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            bev_token: (1, 1, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Rasterize nuScenes BEV image\n",
    "        img = self.input_representation.make_input_representation(\n",
    "            instance_token, sample_token\n",
    "        )   # numpy array (H, W, C)\n",
    "\n",
    "        # 2) Convert ‚Üí torch tensor\n",
    "        bev_img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # (C,H,W)\n",
    "        bev_img = bev_img.unsqueeze(0).to(self.device)                    # (1,C,H,W)\n",
    "\n",
    "        # 3) Normalize (IMPORTANT ‚Äî like ImageNet)\n",
    "        bev_img = (bev_img - bev_img.mean()) / (bev_img.std() + 1e-6)\n",
    "\n",
    "        # 4) Extract embedding using CNN backbone\n",
    "        bev_emb = self.backbone(bev_img)   # (1, d_model)\n",
    "\n",
    "        # 5) Convert to a single transformer token ‚Üí (1,1,d_model)\n",
    "        bev_token = bev_emb.unsqueeze(1)\n",
    "\n",
    "        return bev_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_encoder = BEVEncoder(\n",
    "    input_representation=mtp_input_representation,\n",
    "    d_model=128,\n",
    "    arch=\"resnet18\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# OBTENER TOKEN PARA UN SAMPLE\n",
    "bev_token = bev_encoder(instance_token, sample_token)\n",
    "print(bev_token.shape)\n",
    "\n",
    "# la sortida hauria de ser: torch.Size([1, 1, 128])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bev_embedding(self, instance_token, sample_token):\n",
    "    \n",
    "    # 1. Rasterize BEV image (you already do this)\n",
    "    img = self.input_representation.make_input_representation(\n",
    "        instance_token, sample_token\n",
    "    )  # numpy array (H, W, C)\n",
    "\n",
    "    img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # (C, H, W)\n",
    "    img = img.unsqueeze(0).to(self.device)  # (1, C, H, W)\n",
    "\n",
    "    # 2. Backbone CNN ‚Üí embedding\n",
    "    bev_emb = self.bev_backbone(img)        # (1, d_model)\n",
    "\n",
    "    # 3. Convert to transformer token\n",
    "    bev_token = bev_emb.unsqueeze(1)        # (1, 1, d_model)\n",
    "\n",
    "    return bev_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreguem tots els embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.cat([bev_emb, agent_emb, lane_emb], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despr√©s nom√©s fa falta el model transformer i el head de predicci√≥ de trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sine-cosine positional encoding from the \"Attention is All You Need\" paper.\n",
    "    Adds position information to each token in the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # shape -> (1, max_len, d_model) to broadcast over batch\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Add positional encoding to the input embeddings\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based trajectory prediction model.\n",
    "\n",
    "    Inputs:\n",
    "        - token_embeddings: (batch_size, seq_len, d_model)\n",
    "          Sequence of tokens from:\n",
    "            * 1 BEV embedding\n",
    "            * N agent embeddings\n",
    "            * M lane embeddings\n",
    "\n",
    "    Outputs:\n",
    "        - trajectories: (batch_size, num_modes, num_steps, 2)\n",
    "        - mode_logits (optional): (batch_size, num_modes)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        num_modes: int = 1,\n",
    "        num_steps: int = 12,\n",
    "        use_mode_head: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_modes = num_modes\n",
    "        self.num_steps = num_steps\n",
    "        self.use_mode_head = use_mode_head\n",
    "\n",
    "        # Positional encoding for the sequence tokens\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=256)\n",
    "\n",
    "        # Transformer encoder (no decoder needed, we just encode the scene)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,   # input/output shape = (batch, seq, d_model)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # We will use the \"scene token\" representation to decode trajectories.\n",
    "        # Typically we take the first token (e.g. BEV or a special [CLS]-like token).\n",
    "        self.scene_pool = \"first\"  # could be 'mean' if you want average pooling\n",
    "\n",
    "        # Head that maps the scene embedding to future trajectories\n",
    "        # Output size = num_modes * num_steps * 2 (x, y)\n",
    "        self.traj_head = nn.Linear(d_model, num_modes * num_steps * 2)\n",
    "\n",
    "        # Optional mode classification head (for multi-modal prediction)\n",
    "        if use_mode_head and num_modes > 1:\n",
    "            self.mode_head = nn.Linear(d_model, num_modes)\n",
    "        else:\n",
    "            self.mode_head = None\n",
    "\n",
    "    def _pool_scene_token(self, encoded_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        encoded_tokens: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            scene_emb: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        if self.scene_pool == \"first\":\n",
    "            # Take the first token (assumed to be the BEV / global context token)\n",
    "            scene_emb = encoded_tokens[:, 0, :]\n",
    "        elif self.scene_pool == \"mean\":\n",
    "            scene_emb = encoded_tokens.mean(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scene_pool mode: {self.scene_pool}\")\n",
    "        return scene_emb\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        token_embeddings: torch.Tensor,\n",
    "        src_key_padding_mask: torch.Tensor | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_embeddings: (batch_size, seq_len, d_model)\n",
    "            src_key_padding_mask: optional boolean mask of shape (batch_size, seq_len)\n",
    "                True for positions that should be masked (i.e., padding tokens).\n",
    "\n",
    "        Returns:\n",
    "            trajectories: (batch, num_modes, num_steps, 2)\n",
    "            mode_logits (or None): (batch, num_modes)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Add positional encoding\n",
    "        x = self.pos_encoder(token_embeddings)  # (B, T, D)\n",
    "\n",
    "        # 2) Transformer encoder\n",
    "        # If using src_key_padding_mask, it must be (B, T)\n",
    "        encoded = self.transformer_encoder(\n",
    "            x,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )  # (B, T, D)\n",
    "\n",
    "        # 3) Pool scene representation (e.g. from first token)\n",
    "        scene_emb = self._pool_scene_token(encoded)  # (B, D)\n",
    "\n",
    "        # 4) Trajectory prediction head\n",
    "        traj_flat = self.traj_head(scene_emb)  # (B, num_modes * num_steps * 2)\n",
    "        trajectories = traj_flat.view(\n",
    "            -1, self.num_modes, self.num_steps, 2\n",
    "        )  # (B, M, T, 2)\n",
    "\n",
    "        # 5) Optional mode logits for multi-modal weighting\n",
    "        mode_logits = None\n",
    "        if self.mode_head is not None:\n",
    "            mode_logits = self.mode_head(scene_emb)  # (B, num_modes)\n",
    "\n",
    "        return trajectories, mode_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiModalTrajectoryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal loss for trajectory prediction:\n",
    "      - Picks the best mode based on L2 distance (ADE)\n",
    "      - Applies regression loss on that mode (Smooth L1)\n",
    "      - Applies cross entropy on mode logits\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regression_weight=1.0, classification_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.reg_weight = regression_weight\n",
    "        self.cls_weight = classification_weight\n",
    "        self.reg_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, trajectories, mode_logits, gt_future):\n",
    "        \"\"\"\n",
    "        trajectories: (B, M, T, 2)\n",
    "        mode_logits:  (B, M)\n",
    "        gt_future:    (B, T, 2)\n",
    "        \"\"\"\n",
    "\n",
    "        # Batch size, number of models, y number of timesteps. \n",
    "        B, M, T, _ = trajectories.shape\n",
    "\n",
    "        # ---- STEP 1: Compute ADE for each mode ----\n",
    "        # (B, M, T, 2) - (B, 1, T, 2)\n",
    "        diff = trajectories - gt_future.unsqueeze(1) \n",
    "        l2 = torch.norm(diff, dim=-1)              # (B, M, T)\n",
    "        ade = l2.mean(dim=-1)                      # (B, M)\n",
    "\n",
    "        # ---- STEP 2: Select best mode per sample ----\n",
    "        best_mode = ade.argmin(dim=1)              # (B,)\n",
    "\n",
    "        # ---- STEP 3: Regression loss on best mode ----\n",
    "        best_traj = trajectories[torch.arange(B), best_mode]  # (B, T, 2)\n",
    "        reg_loss = self.reg_loss_fn(best_traj, gt_future)\n",
    "\n",
    "        # ---- STEP 4: Classification loss ----\n",
    "        cls_loss = F.cross_entropy(mode_logits, best_mode)\n",
    "\n",
    "        # ---- STEP 5: Total loss ----\n",
    "        total_loss = self.reg_weight * reg_loss + self.cls_weight * cls_loss\n",
    "\n",
    "        return total_loss, reg_loss.detach(), cls_loss.detach(), best_mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = TrajectoryTransformer(\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    num_modes=3,\n",
    "    num_steps=12,\n",
    "    use_mode_head=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_fn = MultiModalTrajectoryLoss()\n",
    "\n",
    "for tokens, gt_future in dataloader:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    trajectories, mode_logits = model(tokens)\n",
    "\n",
    "    loss, reg_loss, cls_loss, best_mode = loss_fn(\n",
    "        trajectories,\n",
    "        mode_logits,\n",
    "        gt_future\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Total: {loss:.3f} | Reg: {reg_loss:.3f} | Cls: {cls_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lane-Deviation-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtienes la referencia de la lane por la que circula el agente (map_api.get_lane) y se calcula en el entrenamiento la distancia del punto predicho a la polil√≠nea del carril. As√≠ si te sales castigas al modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTANCIA DE UN PUNTO A UN SEGMENTO\n",
    "import torch\n",
    "def point_to_segment_distance(p, a, b):\n",
    "    \"\"\"\n",
    "    p: tensor (..., 2), punto\n",
    "    a, b: tensores (2,), extremos del segmento\n",
    "    Devuelve distancia m√≠nima punto-segmento\n",
    "    \"\"\"\n",
    "    ap = p - a\n",
    "    ab = b - a\n",
    "    ab_norm = torch.sum(ab * ab)\n",
    "\n",
    "    t = torch.clamp(torch.sum(ap * ab) / (ab_norm + 1e-8), 0., 1.)\n",
    "    proj = a + t * ab\n",
    "    return torch.norm(p - proj, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTANCIA DE PUNTO A POLIL√çNEA\n",
    "\n",
    "def point_to_polyline_distance(p, polyline):\n",
    "    \"\"\"\n",
    "    p: tensor (..., 2)\n",
    "    polyline: tensor (N, 2)\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i in range(polyline.shape[0] - 1):\n",
    "        a = polyline[i]\n",
    "        b = polyline[i+1]\n",
    "        dist = point_to_segment_distance(p, a, b)\n",
    "        distances.append(dist)\n",
    "    return torch.stack(distances).min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULAR LA LOSS PARA UNA SOLA TRAYECTORIA\n",
    "\n",
    "def lane_deviation_loss_single(traj_global, lane_polyline):\n",
    "    \"\"\"\n",
    "    traj_global: tensor (T, 2)\n",
    "    lane_polyline: tensor (N, 2)\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for t in range(traj_global.shape[0]):\n",
    "        pt = traj_global[t]\n",
    "        dist = point_to_polyline_distance(pt, lane_polyline)\n",
    "        distances.append(dist)\n",
    "    return torch.stack(distances).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MTPLoss:\n",
    "    \"\"\" Computes the loss for the MTP model. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_modes: int,\n",
    "                 regression_loss_weight: float = 1.,\n",
    "                 angle_threshold_degrees: float = 5.,\n",
    "                 lane_loss_weight=1.0, \n",
    "                 helper=None):\n",
    "        \"\"\"\n",
    "        Inits MTP loss.\n",
    "        :param num_modes: How many modes are being predicted for each agent.\n",
    "        :param regression_loss_weight: Coefficient applied to the regression loss to\n",
    "            balance classification and regression performance.\n",
    "        :param angle_threshold_degrees: Minimum angle needed between a predicted trajectory\n",
    "            and the ground to consider it a match.\n",
    "        \"\"\"\n",
    "        self.num_modes = num_modes\n",
    "        self.num_location_coordinates_predicted = 2  # We predict x, y coordinates at each timestep.\n",
    "        self.regression_loss_weight = regression_loss_weight\n",
    "        self.angle_threshold = angle_threshold_degrees\n",
    "        self.lane_loss_weight = lane_loss_weight\n",
    "        self.helper = helper\n",
    "\n",
    "    def _get_trajectory_and_modes(self,\n",
    "                                  model_prediction: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Splits the predictions from the model into mode probabilities and trajectory.\n",
    "        :param model_prediction: Tensor of shape [batch_size, n_timesteps * n_modes * 2 + n_modes].\n",
    "        :return: Tuple of tensors. First item is the trajectories of shape [batch_size, n_modes, n_timesteps, 2].\n",
    "            Second item are the mode probabilities of shape [batch_size, num_modes].\n",
    "        \"\"\"\n",
    "        mode_probabilities = model_prediction[:, -self.num_modes:].clone()\n",
    "\n",
    "        desired_shape = (model_prediction.shape[0], self.num_modes, -1, self.num_location_coordinates_predicted)\n",
    "        trajectories_no_modes = model_prediction[:, :-self.num_modes].clone().reshape(desired_shape)\n",
    "\n",
    "        return trajectories_no_modes, mode_probabilities\n",
    "\n",
    "    @staticmethod\n",
    "    def _angle_between(ref_traj: torch.Tensor,\n",
    "                       traj_to_compare: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Computes the angle between the last points of the two trajectories.\n",
    "        The resulting angle is in degrees and is an angle in the [0; 180) interval.\n",
    "        :param ref_traj: Tensor of shape [n_timesteps, 2].\n",
    "        :param traj_to_compare: Tensor of shape [n_timesteps, 2].\n",
    "        :return: Angle between the trajectories.\n",
    "        \"\"\"\n",
    "\n",
    "        EPSILON = 1e-5\n",
    "\n",
    "        if (ref_traj.ndim != 2 or traj_to_compare.ndim != 2 or\n",
    "                ref_traj.shape[1] != 2 or traj_to_compare.shape[1] != 2):\n",
    "            raise ValueError('Both tensors should have shapes (-1, 2).')\n",
    "\n",
    "        if torch.isnan(traj_to_compare[-1]).any() or torch.isnan(ref_traj[-1]).any():\n",
    "            return 180. - EPSILON\n",
    "\n",
    "        traj_norms_product = float(torch.norm(ref_traj[-1]) * torch.norm(traj_to_compare[-1]))\n",
    "\n",
    "        # If either of the vectors described in the docstring has norm 0, return 0 as the angle.\n",
    "        if math.isclose(traj_norms_product, 0):\n",
    "            return 0.\n",
    "\n",
    "        # We apply the max and min operations below to ensure there is no value\n",
    "        # returned for cos_angle that is greater than 1 or less than -1.\n",
    "        # This should never be the case, but the check is in place for cases where\n",
    "        # we might encounter numerical instability.\n",
    "        dot_product = float(ref_traj[-1].dot(traj_to_compare[-1]))\n",
    "        angle = math.degrees(math.acos(max(min(dot_product / traj_norms_product, 1), -1)))\n",
    "\n",
    "        if angle >= 180:\n",
    "            return angle - EPSILON\n",
    "\n",
    "        return angle\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ave_l2_norms(tensor: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Compute the average of l2 norms of each row in the tensor.\n",
    "        :param tensor: Shape [1, n_timesteps, 2].\n",
    "        :return: Average l2 norm. Float.\n",
    "        \"\"\"\n",
    "        l2_norms = torch.norm(tensor, p=2, dim=2)\n",
    "        avg_distance = torch.mean(l2_norms)\n",
    "        return avg_distance.item()\n",
    "\n",
    "    def _compute_angles_from_ground_truth(self, target: torch.Tensor,\n",
    "                                          trajectories: torch.Tensor) -> List[Tuple[float, int]]:\n",
    "        \"\"\"\n",
    "        Compute angle between the target trajectory (ground truth) and the predicted trajectories.\n",
    "        :param target: Shape [1, n_timesteps, 2].\n",
    "        :param trajectories: Shape [n_modes, n_timesteps, 2].\n",
    "        :return: List of angle, index tuples.\n",
    "        \"\"\"\n",
    "        angles_from_ground_truth = []\n",
    "        for mode, mode_trajectory in enumerate(trajectories):\n",
    "            # For each mode, we compute the angle between the last point of the predicted trajectory for that\n",
    "            # mode and the last point of the ground truth trajectory.\n",
    "            angle = self._angle_between(target[0], mode_trajectory)\n",
    "\n",
    "            angles_from_ground_truth.append((angle, mode))\n",
    "        return angles_from_ground_truth\n",
    "\n",
    "    def _compute_best_mode(self,\n",
    "                           angles_from_ground_truth: List[Tuple[float, int]],\n",
    "                           target: torch.Tensor, trajectories: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Finds the index of the best mode given the angles from the ground truth.\n",
    "        :param angles_from_ground_truth: List of (angle, mode index) tuples.\n",
    "        :param target: Shape [1, n_timesteps, 2]\n",
    "        :param trajectories: Shape [n_modes, n_timesteps, 2]\n",
    "        :return: Integer index of best mode.\n",
    "        \"\"\"\n",
    "\n",
    "        # We first sort the modes based on the angle to the ground truth (ascending order), and keep track of\n",
    "        # the index corresponding to the biggest angle that is still smaller than a threshold value.\n",
    "        angles_from_ground_truth = sorted(angles_from_ground_truth)\n",
    "        max_angle_below_thresh_idx = -1\n",
    "        for angle_idx, (angle, mode) in enumerate(angles_from_ground_truth):\n",
    "            if angle <= self.angle_threshold:\n",
    "                max_angle_below_thresh_idx = angle_idx\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # We choose the best mode at random IF there are no modes with an angle less than the threshold.\n",
    "        if max_angle_below_thresh_idx == -1:\n",
    "            best_mode = random.randint(0, self.num_modes - 1)\n",
    "\n",
    "        # We choose the best mode to be the one that provides the lowest ave of l2 norms between the\n",
    "        # predicted trajectory and the ground truth, taking into account only the modes with an angle\n",
    "        # less than the threshold IF there is at least one mode with an angle less than the threshold.\n",
    "        else:\n",
    "            # Out of the selected modes above, we choose the final best mode as that which returns the\n",
    "            # smallest ave of l2 norms between the predicted and ground truth trajectories.\n",
    "            distances_from_ground_truth = []\n",
    "\n",
    "            for angle, mode in angles_from_ground_truth[:max_angle_below_thresh_idx + 1]:\n",
    "                norm = self._compute_ave_l2_norms(target - trajectories[mode, :, :])\n",
    "\n",
    "                distances_from_ground_truth.append((norm, mode))\n",
    "\n",
    "            distances_from_ground_truth = sorted(distances_from_ground_truth)\n",
    "            best_mode = distances_from_ground_truth[0][1]\n",
    "\n",
    "        return best_mode\n",
    "\n",
    "    def __call__(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the MTP loss on a batch.\n",
    "        The predictions are of shape [batch_size, n_ouput_neurons of last linear layer]\n",
    "        and the targets are of shape [batch_size, 1, n_timesteps, 2]\n",
    "        :param predictions: Model predictions for batch.\n",
    "        :param targets: Targets for batch.\n",
    "        :return: zero-dim tensor representing the loss on the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_losses = torch.Tensor().requires_grad_(True).to(predictions.device)\n",
    "        trajectories, modes = self._get_trajectory_and_modes(predictions)\n",
    "\n",
    "        for batch_idx in range(predictions.shape[0]):\n",
    "\n",
    "            angles = self._compute_angles_from_ground_truth(target=targets[batch_idx],\n",
    "                                                            trajectories=trajectories[batch_idx])\n",
    "\n",
    "            best_mode = self._compute_best_mode(angles,\n",
    "                                                target=targets[batch_idx],\n",
    "                                                trajectories=trajectories[batch_idx])\n",
    "\n",
    "            best_mode_trajectory = trajectories[batch_idx, best_mode, :].unsqueeze(0)\n",
    "\n",
    "            regression_loss = f.smooth_l1_loss(best_mode_trajectory, targets[batch_idx])\n",
    "\n",
    "            mode_probabilities = modes[batch_idx].unsqueeze(0)\n",
    "            best_mode_target = torch.tensor([best_mode], device=predictions.device)\n",
    "            classification_loss = f.cross_entropy(mode_probabilities, best_mode_target)\n",
    "\n",
    "            loss = classification_loss + self.regression_loss_weight * regression_loss\n",
    "            # ============================================================\n",
    "            # üõ£Ô∏è LANE DEVIATION LOSS\n",
    "            # ============================================================\n",
    "            if self.lane_loss_weight > 0 and self.helper is not None:\n",
    "\n",
    "                instance_token, sample_token = tokens[batch_idx]\n",
    "\n",
    "                ann = self.helper.get_sample_annotation(instance_token, sample_token)\n",
    "                agent_x, agent_y = ann[\"translation\"][:2]\n",
    "\n",
    "                lane_ids = self.helper.map_api.get_lane_ids_in_xy(agent_x, agent_y)\n",
    "\n",
    "                if len(lane_ids) > 0:\n",
    "                    lane_id = lane_ids[0]\n",
    "                    lane_poly = torch.tensor(\n",
    "                        self.helper.map_api.get_lane_centerline(lane_id)[:, :2],\n",
    "                        dtype=best_mode_trajectory.dtype,\n",
    "                        device=best_mode_trajectory.device\n",
    "                    )\n",
    "\n",
    "                    # Convert local -> global\n",
    "                    traj_local = best_mode_trajectory[0]   # (T,2)\n",
    "                    quat = Quaternion(ann[\"rotation\"])\n",
    "\n",
    "                    traj_rot = torch.tensor(\n",
    "                        [quat.rotate((p[0].item(), p[1].item(), 0.0))[:2] for p in traj_local],\n",
    "                        dtype=traj_local.dtype,\n",
    "                        device=traj_local.device\n",
    "                    )\n",
    "\n",
    "                    traj_global = traj_rot + torch.tensor([agent_x, agent_y], device=traj_rot.device)\n",
    "\n",
    "                    # Lane deviation loss\n",
    "                    lane_loss = lane_deviation_loss_single(traj_global, lane_poly)\n",
    "\n",
    "                    loss = loss + self.lane_loss_weight * lane_loss\n",
    "\n",
    "\n",
    "            batch_losses = torch.cat((batch_losses, loss.unsqueeze(0)), 0)\n",
    "\n",
    "        avg_loss = torch.mean(batch_losses)\n",
    "\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canviar quan es truca a la loss per posarli \n",
    "# una weight a la lane loss (canviar a mtp)\n",
    "\n",
    "loss_fn = MTPLoss(\n",
    "    num_modes=num_modes,\n",
    "    regression_loss_weight=1.0,\n",
    "    angle_threshold_degrees=5.,\n",
    "    lane_loss_weight=1.0,   # <-- nuevo\n",
    "    helper=helper           # <-- necesario\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Snap-to-Lane: \n",
    "\n",
    "after the training is done the snap to lane function is used in the prediction to predict the closest point that is IN the lane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyquaternion import Quaternion\n",
    "import numpy as np\n",
    "\n",
    "def get_agent_lane(helper, instance_token, sample_token):\n",
    "    # Posici√≥n del agente en coordenadas globales\n",
    "    annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "    agent_x, agent_y = annotation['translation'][:2]\n",
    "    \n",
    "    lanes = helper.map_api.get_lane_ids_in_xy(agent_x, agent_y)\n",
    "    if len(lanes) == 0:\n",
    "        return None  # No ha encontrado lane (raro, pero posible)\n",
    "    \n",
    "    # Devolvemos la primera para simplificar\n",
    "    return lanes[0]\n",
    "\n",
    "def get_lane_centerline(helper, lane_id):\n",
    "    record = helper.map_api.get_lane(lane_id)\n",
    "    lane_center = helper.map_api.get_lane_centerline(lane_id)\n",
    "    # lane_center es un array Nx2 con la polil√≠nea\n",
    "    return np.array(lane_center[:, :2])\n",
    "\n",
    "def project_point_to_polyline(point, polyline):\n",
    "    px, py = point\n",
    "    min_dist = float('inf')\n",
    "    closest_point = None\n",
    "    \n",
    "    for i in range(len(polyline) - 1):\n",
    "        p1 = polyline[i]\n",
    "        p2 = polyline[i+1]\n",
    "        \n",
    "        v = p2 - p1\n",
    "        w = point - p1\n",
    "        \n",
    "        t = np.dot(w, v) / (np.dot(v, v) + 1e-8)\n",
    "        t = np.clip(t, 0, 1)\n",
    "        \n",
    "        proj = p1 + t * v\n",
    "        dist = np.linalg.norm(point - proj)\n",
    "\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_point = proj\n",
    "            \n",
    "    return closest_point\n",
    "\n",
    "\n",
    "def snap_trajectory_to_lane(global_traj, helper, instance_token, sample_token):\n",
    "    ann = helper.get_sample_annotation(instance_token, sample_token)\n",
    "    x, y = ann[\"translation\"][:2]\n",
    "\n",
    "    lane_ids = helper.map_api.get_lane_ids_in_xy(x, y)\n",
    "    if len(lane_ids) == 0:\n",
    "        return global_traj  # no lane found\n",
    "\n",
    "    lane_id = lane_ids[0]\n",
    "    centerline = helper.map_api.get_lane_centerline(lane_id)[:, :2]\n",
    "\n",
    "    snapped = []\n",
    "    for point in global_traj:\n",
    "        snapped.append(project_point_to_polyline(point, centerline))\n",
    "    return np.array(snapped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add to the generate submision function to pass from the global to a lane aligned position\n",
    "\n",
    "pred_coords_global[mode_idx] = snap_trajectory_to_lane(\n",
    "    pred_coords_global[mode_idx],\n",
    "    helper,\n",
    "    instance_token,\n",
    "    sample_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_notebook(model, dataset, output_path=\"submission.json\"):\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    \n",
    "    # Necesitamos el helper para buscar la pose del agente\n",
    "    helper = dataset.helper \n",
    "\n",
    "    print(f\"üöó Generando submission con conversi√≥n LOCAL -> GLOBAL...\")\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        img, agent_state, _, _ = dataset[i]\n",
    "        \n",
    "        # Recuperar tokens\n",
    "        raw_token = dataset.split[i]\n",
    "        instance_token, sample_token = raw_token.split(\"_\")\n",
    "\n",
    "        # Inferencia\n",
    "        img = img.unsqueeze(0)        \n",
    "        agent_state = agent_state.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(img, agent_state)\n",
    "\n",
    "        # Procesar salida (tu c√≥digo de antes)\n",
    "        total_output_size = pred.shape[1]\n",
    "        num_modes = total_output_size // 25 \n",
    "        num_coords = num_modes * 24\n",
    "        \n",
    "        pred_coords = pred[0, :num_coords]\n",
    "        pred_probs = pred[0, num_coords:]\n",
    "        \n",
    "        # [Num_modos, 12, 2] en coordenadas LOCALES\n",
    "        pred_coords_local = pred_coords.reshape(num_modes, 12, 2).cpu().numpy()\n",
    "\n",
    "        # ============================================================\n",
    "        # üåç TRANSFORMACI√ìN CR√çTICA: LOCAL -> GLOBAL\n",
    "        # ============================================================\n",
    "        \n",
    "        # 1. Obtener la pose actual del agente en el mapa global\n",
    "        sample_annotation = helper.get_sample_annotation(instance_token, sample_token)\n",
    "        translation = sample_annotation['translation'] # [x, y, z] global\n",
    "        rotation = sample_annotation['rotation']       # Quaternion global\n",
    "        \n",
    "        # 2. Convertir a matriz de transformaci√≥n (Local -> Global)\n",
    "        # Nota: transform_matrix espera rotaci√≥n como Quaternion y translaci√≥n\n",
    "        # Pero ojo: MTP predice X,Y (2D). NuScenes es 3D.\n",
    "        \n",
    "        # Manera simplificada de rotar y trasladar vectores 2D:\n",
    "        quaternion = Quaternion(rotation)\n",
    "        \n",
    "        # Creamos un array vac√≠o para las coordenadas globales\n",
    "        pred_coords_global = np.zeros_like(pred_coords_local)\n",
    "\n",
    "        for mode_idx in range(num_modes):\n",
    "            # Cogemos la trayectoria de un modo (Shape: 12, 2)\n",
    "            trajectory_local = pred_coords_local[mode_idx]\n",
    "            \n",
    "            # A. A√±adimos una columna de ceros para Z (necesario para rotaci√≥n 3D)\n",
    "            # Shape se convierte en (12, 3) -> [x, y, 0]\n",
    "            traj_3d = np.hstack([trajectory_local, np.zeros((12, 1))])\n",
    "            \n",
    "            # B. Rotar (El agente mira hacia una direcci√≥n, rotamos los puntos)\n",
    "            # Iteramos punto a punto o usamos vectorizaci√≥n si es posible. \n",
    "            # rotate funciona con vector √∫nico, as√≠ que iteramos para asegurar:\n",
    "            traj_rotated = np.array([quaternion.rotate(p) for p in traj_3d])\n",
    "            \n",
    "            # C. Trasladar (Sumar la posici√≥n global actual del coche)\n",
    "            # Solo sumamos X e Y (√≠ndices 0 y 1)\n",
    "            pred_coords_global[mode_idx, :, 0] = traj_rotated[:, 0] + translation[0]\n",
    "            pred_coords_global[mode_idx, :, 1] = traj_rotated[:, 1] + translation[1]\n",
    "\n",
    "            # ============================================================\n",
    "            # üõ£Ô∏è SNAP-TO-LANE (GLOBAL ‚Üí LANE-ALIGNED)\n",
    "            # ============================================================\n",
    "            pred_coords_global[mode_idx] = snap_trajectory_to_lane(\n",
    "                pred_coords_global[mode_idx],\n",
    "                helper,\n",
    "                instance_token,\n",
    "                sample_token\n",
    "            )\n",
    "            \n",
    "        # ============================================================\n",
    "\n",
    "        # Probabilidades\n",
    "        if num_modes > 1:\n",
    "            probs = torch.nn.functional.softmax(pred_probs, dim=0).cpu().numpy()\n",
    "        else:\n",
    "            probs = np.array([1.0])\n",
    "\n",
    "        prediction_obj = Prediction(\n",
    "            instance=instance_token,\n",
    "            sample=sample_token,\n",
    "            prediction=pred_coords_global, # ¬°USAMOS LAS GLOBALES!\n",
    "            probabilities=probs\n",
    "        )\n",
    "\n",
    "        predictions_list.append(prediction_obj.serialize())\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(predictions_list, f, indent=2)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESTRINGIR EL ESPACIO DE PREDICCI√ìN (LANE CONDITIONED MTP)\n",
    "\n",
    "4Ô∏è‚É£ Restringir el espacio de predicci√≥n (Lane-conditioned MTP)\n",
    "\n",
    "En vez de dejar que el modelo prediga cualquier trayectoria libre, puedes:\n",
    "\n",
    "Generar modos condicionados por la estructura de la lane (ramas, salidas, giros).\n",
    "\n",
    "Hacer que cada ‚Äúmodo‚Äù siga una lane candidate.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "CoverNet + Lattice basado en lanes\n",
    "\n",
    "LaneGCN\n",
    "\n",
    "Wayformer con road graph\n",
    "\n",
    "Aqu√≠ el modelo pr√°cticamente solo puede elegir trayectorias v√°lidas por construcci√≥n.\n",
    "\n",
    "Ventaja:\n",
    "\n",
    "Es la soluci√≥n m√°s elegante acad√©micamente.\n",
    "\n",
    "Desventaja:\n",
    "\n",
    "M√°s trabajo de ingenier√≠a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FER M√âS INTERESSANT EL BIRD EYE VIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Darle la informaci√≥n de la lane (BEV o vector lanes)\n",
    "\n",
    "‚û°Ô∏è La opci√≥n que sugiere tu profe.\n",
    "‚û°Ô∏è Es buena porque el modelo aprende ‚Äúpor s√≠ mismo‚Äù la geometr√≠a del mapa.\n",
    "\n",
    "Formas de hacerlo:\n",
    "\n",
    "Raster BEV completo (lo que estamos montando ahora).\n",
    "\n",
    "Lanes vectorizadas (formato Trajectron++ / VectorNet).\n",
    "\n",
    "A√±adir polil√≠neas directamente como input a un GNN o MLP.\n",
    "\n",
    "Ventaja: no fuerza expl√≠citamente, solo ayuda.\n",
    "Desventaja: el modelo a veces puede seguir equivoc√°ndose."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
